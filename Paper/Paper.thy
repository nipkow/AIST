(*<*)
theory Paper
imports
  Context_Free_Grammar.Chomsky_Normal_Form
  Greibach_Normal_Form.Greibach_Normal_Form
  Sugar
begin
declare [[show_question_marks=false]]
lemma expand_hd_simp2: "expand_hd A (S#Ss) R =
 (let R' = expand_hd A Ss R;
      X = {r \<in> R'. \<exists>w. r = (A, Nt S # w)}
  in R' - X \<union> subst_hd R' X)"
  by simp

(*>*)
text \<open>
\section{Introduction}

The formalization is unified in the sense that some of the topics that were 
previously formalized independently (and in different provers) are now
unified in a single formalization.
%rebase LL1 parser on CFG!

Of course only textbook+, no parsing (separate).

\subsection{Related Work}

\subsection{Isabelle Notation} \label{sec:isabelle}

Isabelle is based on a fragment of higher-order logic. It supports core
concepts commonly found in functional programming languages.

The notation \<open>t :: \<tau>\<close> indicates that term \<open>t\<close> has type
\<open>\<tau>\<close>. Basic types include @{typ bool} and @{typ nat}, while type variables are written @{typ 'a}, @{typ 'b} etc.
Pairs are expressed
as \mbox{@{term "(a, b)"}}, and triples as @{term "(a, b, c)"}, and so forth.
%Functions {term fst} and {term snd} return the first and second components of a pair,
%while the operator {term "(\<times>)"} is used for pairs at the type level.
Most type constructors are written postfix, such as @{typ "'a set"} and  @{typ "'a list"}, and the function
space arrow is \<open>\<Rightarrow>\<close>. Function @{const set} converts a list to a set.

Lists are constructed from the empty list @{term "[]"} using the infix cons-operator @{term "(#)"}.
The operator @{term "(@)"} appends two lists, @{term "length xs"} denotes the length of @{term xs},
@{term "xs!n"} returns the \<open>n\<close>-th element of the list @{term xs} (starting with @{prop "n=(0::nat)"}).
%and @{term "take n xs"} is the prefix of length \<open>n\<close> of \<open>xs\<close>.

Algebraic data types are defined using the \isakeyword{datatype} keyword. A predefined data type:
\begin{quote}
@{datatype option}
\end{quote}
The notation \mbox{\<open>\<lbrakk>A\<^sub>1, \<dots>, A\<^sub>n\<rbrakk> \<Longrightarrow> B\<close>} denotes an implication with premises \<open>A\<^sub>1\<close>, \ldots, \<open>A\<^sub>n\<close> and conclusion \<open>B\<close>.
Equality on type @{type bool} denotes logical equivalence.


\section{The Basic Theory}

\subsection{Context-Free Grammars}

All our types are parameterized by type variables \<open>'n\<close> and \<open>'t\<close>, the types of nonterminals and terminals.
Symbols are a tagged union type:
\begin{quote}
@{datatype sym}
\end{quote}
For compactness we usually drop the \<open>'n\<close> and \<open>'t\<close> parameters everywhere,
e.g.\ we write \<open>sym\<close> instead of \<open>('n,'t)sym\<close>.

A production is a pair of \<open>A :: 'n\<close> and a \<open>w :: sym list\<close>. We use the following abbreviations:
\begin{quote}
\<open>syms = sym list\<close> \quad \<open>prod = ('n \<times> syms)\<close> \quad \<open>Prods = prod set\<close>
\end{quote}
Our theory is primarily based on sets of productions rather than grammars:
the start symbol is irrelevant most of the time.
\emph{For succinctness, we use \concept{grammar} to refer to a set (or list) of productions.}
Moreover, we only restrict to finite sets of productions when necessary.
However, some constructions are hard or impossible even for finite grammars,
unless we have some order on them: if we need to create new variables that appear in the output,
the order in which a grammar is processed is crucial. Therefore we work with \emph{lists} of
productions half of the time and define \<open>prods = prod list\<close>.
We work with sets whenever possible (because of their abstractness) and with lists only if necessary.
Function \<^const>\<open>set\<close> converts in one direction, but we cannot convert from sets to lists
in a computable manner (unless we impose some order on something).

The identifier \<open>P\<close> is reserved for sets of productions.
Every \<open>P\<close> induces a single step derivation relation on \<open>syms\<close> in the standard manner:
\begin{quote}
@{thm derive.intros}
\end{quote}
Its transitive-reflexive closure is denoted by @{prop "P \<turnstile> u \<Rightarrow>* w"}.

The language of some nonterminal \<open>A\<close> generated by \<open>P\<close> is easily defined:
\begin{quote}
@{thm Lang_def}
\end{quote}
There is also the abbreviation @{abbrev "lang ps A"}. This is an example of our convention:
functions named \<open>Fn\<close> (starting with a capital letter) operate on sets, the lower case \<open>fn\<close>
is its analogue on lists. We usually present only one of the two versions and silently use the other one
(if it exists).

Fresh names are generated by these functions: @{term "fresh0 X"} (generates a name not in \<open>X\<close>)
and @{term "freshs X As"} (generates variants of the names in list \<open>As\<close> not in \<open>X\<close>).


\subsection{Chomsky Normal Form}

Naturally we have shown that a finite grammar has an equivalent (modulo \<open>[]\<close>) finite grammar in Chomsky Normal Form:
\begin{quote}
@{thm CNF_def}\\
@{thm cnf_exists}
\end{quote}
A constructive formalization is due to Hofmann \<^cite>\<open>JHofmann\<close>.
Ramos \cite{RamosAMQ} also formalized four applications, two of which we also formalized
(non-context freeness of $a^nb^nc^n$ and non-closedness of context-free languages under intersection)
using only 10--15\% of the number of lines.


\subsection{Pushdown Automata}
\subsection{Pumping Lemma}
\subsection{Automata}

2DFA AFA?

\section{Pre*}

\section{Greibach}%AY

\begin{definition}
A grammar \<open>R\<close> is in \emph{(head) Greibach normal form (GNF)} if
every right-hand side in \<open>R\<close> starts with a terminal symbol.
Formally,
\begin{quote}
@{def GNF_hd}
\end{quote}
\end{definition}

The main result of this section is the construction of the function @{const gnf_hd},
which turns a grammar into GNF while preserving the language modulo \<open>\<epsilon>\<close>.
\begin{theorem}
@{thm GNF_hd_gnf_hd}\\
@{thm Lang_gnf_hd}
\end{theorem}

The main ingredient of @{const gnf_hd} is the removal of \emph{direct left recursions},
i.e., rules of form \<open>A \<rightarrow> A v \<in> R\<close>.
Let \<open>V\<close> collect all such \<open>v\<close>,
and let \<open>U\<close> collect all \<open>u\<close> of \<open>A \<rightarrow> u \<in> R\<close> that does not start with \<open>A\<close>.
Then the language of \<open>A\<close> is \<open>U \<union> U V\<^sup>+\<close>;
hence we introduce a fresh nonterminal \<open>A'\<close> whose language is \<open>V\<^sup>+\<close>.

\begin{quote}
@{def rrec_of_lrec}
\end{quote}

\begin{quote}
@{def solve_lrec[solve_lrec_def[unfolded rm_lrec_def]]}
\end{quote}

The formalization is almost the same as the textual description,
except that
if @{prop \<open>V = {}\<close>}, then @{term "solve_lrec A A'"} returns the original \<open>A\<close>-productions
(excluding useless \<open>A \<rightarrow> A\<close> production).
This optimization is not present in \<^cite>\<open>HopcroftU79\<close>,
but their Example 4.10 performs this optimization implicitly.


\begin{quote}
@{abbrev "subst_hd R X"}
\end{quote}

\begin{quote}
@{fun expand_hd[expand_hd.simps(1) expand_hd_simp2]}
\end{quote}

\begin{quote}
@{fun solve_tri}
\end{quote}

\begin{quote}
@{fun expand_tri}
\end{quote}

\begin{definition}
@{def gnf_hd}
\end{definition}


\section{Chomsky-Sch\"utzenberger}

\section{Parikh}
\<close>
(*<*)
end
(*>*)